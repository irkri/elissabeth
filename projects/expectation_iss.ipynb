{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectation ISS\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathbb{E}[\\operatorname{ISS}(u)]&=\\sum_{t_1<\\dotsc<t_p\\leq T}\\mathbb{E}[u_{t_1}\\cdots u_{t_p}]\\\\\n",
    "&=\\sum_{t_1<\\dotsc<t_p\\leq T}\\sum_{\\pi\\in P_p^2}\\prod_{(i,j)\\in\\pi}\\mathbb{E}[u_{t_i}u_{t_j}]\\\\\n",
    "&\\leq{T\\choose p}{p\\choose 2}e^{-\\alpha\\frac{p}{2}}\n",
    "\\end{align*}\n",
    "$$\n",
    "for $\\operatorname{Cov}(u_s,u_t)=e^{-\\alpha|t-s|}$ and $\\alpha>0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing import Sequence\n",
    "from itertools import combinations\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "from sainomore import Elissabeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairings(lst: Sequence[int] | int):\n",
    "    if isinstance(lst, int):\n",
    "        lst = list(range(lst))\n",
    "    else:\n",
    "        lst = list(lst)\n",
    "    if len(lst) < 2:\n",
    "        yield []\n",
    "        return\n",
    "    a = lst[0]\n",
    "    for i in range(1, len(lst)):\n",
    "        pair = (a, lst[i])\n",
    "        for rest in pairings(lst[1:i]+lst[i+1:]):\n",
    "            yield [pair] + rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation(cov: torch.Tensor, T: int, p: int) -> float:\n",
    "    if p % 2 == 1:\n",
    "        return 0\n",
    "    expect = 0\n",
    "    for t in combinations(list(range(T)), p):\n",
    "        for pairing in pairings(t):\n",
    "            prod = 1\n",
    "            for pair in pairing:\n",
    "                prod *= float(cov[pair[0], pair[1]])\n",
    "            expect += prod\n",
    "    return expect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expectation_upper_bound(T: int, p: int, alpha: float) -> float:\n",
    "    if alpha > 0:\n",
    "        return math.comb(T, p) * math.comb(p, 2) * math.exp(-alpha*p/2)\n",
    "    return math.comb(T, p) * math.comb(p, 2) * math.exp(-alpha*p/2*(T-p/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_cov(T: int, alpha: float) -> torch.Tensor:\n",
    "    a = torch.eye(T)\n",
    "    for i in range(T):\n",
    "        a[*torch.tril_indices(T, T, -i)] = - i*alpha\n",
    "        a[*torch.triu_indices(T, T, i)] = - i*alpha\n",
    "    return torch.exp(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation_upper_bound(100, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expectation(exp_cov(100, 1), 100, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Norm of Elissabeth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(T: int, sum_norm: bool, layer_norm: bool) -> Elissabeth:\n",
    "    config = {\n",
    "        \"input_vocab_size\": 1,\n",
    "        \"input_type\": \"vector\",\n",
    "        \"context_length\": T,\n",
    "        \"n_layers\": 5,\n",
    "        \"d_hidden\": 10,\n",
    "        \"layer_norm\": layer_norm,\n",
    "        \"residual_stream\": True,\n",
    "\n",
    "        \"lengths\": [1,2,3,4,5],\n",
    "        \"d_values\": 10,\n",
    "        \"values_2D\": False,\n",
    "        \"v_norm\": False,\n",
    "        \"sum_normalization\": sum_norm,\n",
    "    }\n",
    "    model = Elissabeth.build(config)\n",
    "    torch.nn.init.xavier_normal_(model.get_parameter(\"embedding.weight\"))\n",
    "    torch.nn.init.ones_(model.get_parameter(\"unembedding.weight\"))\n",
    "    state_dict = model.state_dict()\n",
    "    for l in range(5):\n",
    "        for p in range(5):\n",
    "            state_dict[f\"layers.{l}.levels.{p}.P_V.transform.weight\"] = (\n",
    "                torch.cat((p+1)*(torch.eye(10),))\n",
    "            )\n",
    "        state_dict[f\"layers.{l}.W_O\"] = torch.eye(10).unsqueeze(1)\n",
    "        state_dict[f\"layers.{l}.W_H\"] = torch.ones(5).unsqueeze(1)\n",
    "    model.load_state_dict(state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_x(T: int, n: int, alpha: float) -> torch.Tensor:\n",
    "    dist = torch.distributions.multivariate_normal.MultivariateNormal(\n",
    "        torch.zeros((T,)), exp_cov(T, alpha)\n",
    "    )\n",
    "    return dist.sample(torch.Size((n, ))).unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts = [10, 100, 1000]\n",
    "alphas = [10, 1, 1e-1, 1e-2, 1e-3]\n",
    "n = 100\n",
    "n_layers = 5\n",
    "n_lengths = 5\n",
    "\n",
    "norm = torch.zeros((4, n, n_layers*n_lengths, len(alphas), len(Ts)))\n",
    "bounds = torch.zeros((n_lengths, len(alphas), len(Ts)))\n",
    "\n",
    "for it, t in enumerate(Ts):\n",
    "    model = get_model(t, False, False)\n",
    "    model_iss = get_model(t, True, False)\n",
    "    model_layer = get_model(t, False, True)\n",
    "    model_iss_layer = get_model(t, True, True)\n",
    "    model.attach_all_hooks()\n",
    "    model_iss.attach_all_hooks()\n",
    "    model_layer.attach_all_hooks()\n",
    "    model_iss_layer.attach_all_hooks()\n",
    "    for ai, a in enumerate(alphas):\n",
    "        model(sample_x(t, n, a))\n",
    "        model_iss(sample_x(t, n, a))\n",
    "        model_layer(sample_x(t, n, a))\n",
    "        model_iss_layer(sample_x(t, n, a))\n",
    "        for l in range(n_layers):\n",
    "            for p in range(n_lengths):\n",
    "                bounds[p, ai, it] = expectation_upper_bound(t, p+1, a)\n",
    "                iss = model.get_hook(f\"layers.{l}.levels.{p}\", \"iss\").fwd.squeeze()\n",
    "                norm[0, :, l*n_layers+p, ai, it] = iss[:, -1, :].norm(dim=-1)\n",
    "                iss = model_iss.get_hook(f\"layers.{l}.levels.{p}\", \"iss\").fwd.squeeze()\n",
    "                norm[1, :, l*n_layers+p, ai, it] = iss[:, -1, :].norm(dim=-1)\n",
    "                iss = model_layer.get_hook(f\"layers.{l}.levels.{p}\", \"iss\").fwd.squeeze()\n",
    "                norm[2, :, l*n_layers+p, ai, it] = iss[:, -1, :].norm(dim=-1)\n",
    "                iss = model_iss_layer.get_hook(f\"layers.{l}.levels.{p}\", \"iss\").fwd.squeeze()\n",
    "                norm[3, :, l*n_layers+p, ai, it] = iss[:, -1, :].norm(dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(4, len(Ts), figsize=(18, 9), sharex=True, sharey=True)\n",
    "\n",
    "cm = plt.get_cmap(\"tab10\")\n",
    "for i in range(len(alphas)):\n",
    "    for k in range(4):\n",
    "        ax[k, 0].plot(list(range(25)), norm[k, :, :, i, 0].mean(dim=0), \"-x\", label=rf\"$\\alpha={alphas[i]}$\", color=cm(i))\n",
    "        ax[k, 1].plot(list(range(25)), norm[k, :, :, i, 1].mean(dim=0), \"-x\", label=rf\"$\\alpha={alphas[i]}$\", color=cm(i))\n",
    "        ax[k, 2].plot(list(range(25)), norm[k, :, :, i, 2].mean(dim=0), \"-x\", label=rf\"$\\alpha={alphas[i]}$\", color=cm(i))\n",
    "        ax[k, 0].grid()\n",
    "        ax[k, 1].grid()\n",
    "        ax[k, 2].grid()\n",
    "    ax[0, 0].plot(list(range(1, 5)), bounds[1:, i, 0], \"--\", color=cm(i))\n",
    "    ax[0, 1].plot(list(range(1, 5)), bounds[1:, i, 1], \"--\", color=cm(i))\n",
    "    ax[0, 2].plot(list(range(1, 5)), bounds[1:, i, 2], \"--\", color=cm(i))\n",
    "\n",
    "ax[0, 0].set_ylim(1e-5, 1e10)\n",
    "ax[0, 2].legend(loc=\"best\", fontsize=15)\n",
    "ax[0, 0].set_title(\"$T=10$\", fontsize=20)\n",
    "ax[0, 1].set_title(\"$T=100$\", fontsize=20)\n",
    "ax[0, 2].set_title(\"$T=1000$\", fontsize=20)\n",
    "ax[0, 0].set_ylabel(\"no norm\", fontsize=17)\n",
    "ax[1, 0].set_ylabel(\"sum norm\", fontsize=17)\n",
    "ax[2, 0].set_ylabel(\"layer norm\", fontsize=17)\n",
    "ax[3, 0].set_ylabel(\"sum+layer norm\", fontsize=17)\n",
    "for axis in ax.flatten():\n",
    "    axis.tick_params(axis='x', which='major', width=1, length=5, labelsize=12)\n",
    "    axis.tick_params(axis='x', which='minor', labelsize=0)\n",
    "\n",
    "    axis.set_xticks([l*n_lengths for l in range(0, n_layers+1)])\n",
    "    axis.set_xticks(list(range(n_lengths*n_layers)), minor=True)\n",
    "    axis.set_xticklabels([f\"L {l}\" for l in range(n_layers+1)], fontsize=15)\n",
    "    axis.set_yscale(\"log\")\n",
    "fig.tight_layout()\n",
    "plt.savefig(\n",
    "    \"norm_elissabeth.pdf\",\n",
    "    facecolor=(0, 0, 0, 0),\n",
    "    bbox_inches=\"tight\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sainomore-JMySAefd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
