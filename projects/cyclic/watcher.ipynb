{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sainomore.xai import ElissabethWatcher, get_alphabet_projection\n",
    "\n",
    "from data import cyclic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watcher = ElissabethWatcher.load(model_id, on_cpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state_dict = watcher.model.state_dict()\n",
    "# # state_dict[\"layers.0.W_H\"] = torch.tensor([[0, 1, 1, 1, 1]])\n",
    "# state_dict[\"layers.0.W_O\"] = torch.tensor([[[1, 1, 1, 1, 1, 1]]])\n",
    "# watcher.model.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = watcher.plot_parameter_matrix(\n",
    "    \"layers.0.W_H\",\n",
    "    figsize=(3, 3),\n",
    ")\n",
    "fig, ax = watcher.plot_parameter_matrix(\n",
    "    \"layers.0.W_O\",\n",
    "    reduce_dims={2: 0},\n",
    "    append_dims=(0,1,),\n",
    "    figsize=(10, 3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = watcher.plot_parameter_matrix(\n",
    "    \"embedding.weight\",\n",
    "    figsize=(4, 4),\n",
    ")\n",
    "fig, ax = watcher.plot_parameter_matrix(\n",
    "    \"unembedding.weight\",\n",
    "    figsize=(4, 4),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example based"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = cyclic(\n",
    "    1,\n",
    "    length=5,\n",
    "    characters=6,\n",
    ")\n",
    "print(x)\n",
    "print(x[x!=0])\n",
    "print(y, watcher.model(x)[0,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(6)\n",
    "v = watcher.get_values(x)\n",
    "\n",
    "fig, ax = plt.subplots(4, 2, sharex=True, sharey=True, figsize=(5, 6))\n",
    "for i in range(4):\n",
    "    for j in range(2):\n",
    "        ax[i,j].bar(\n",
    "            x,\n",
    "            v[i,j,0].numpy(),\n",
    "            color=[\"red\" if k == v[i,j,0].argmax() else \"orange\" for k in range(6)],\n",
    "            width=.75,\n",
    "        )\n",
    "        ax[i,j].set_xticks(x)\n",
    "        ax[i,j].set_xticklabels(x.numpy(), fontsize=15)\n",
    "        ax[i,j].set_yticks(torch.linspace(-5, 2, 8))\n",
    "        ax[i,j].set_yticklabels([\"-5\", \"\", \"\", \"\", \"\", \"0\", \"\", \"2\"])\n",
    "        ax[i,j].set_ylim(-5, 2)\n",
    "        ax[i,j].grid()\n",
    "ax[0, 0].set_title(\"$v^{[1]}$\", fontsize=20)\n",
    "ax[0, 1].set_title(\"$v^{[2]}$\", fontsize=20)\n",
    "# plt.savefig(\n",
    "#     \"cyclic_arctic_values.pdf\",\n",
    "#     facecolor=(0, 0, 0, 0),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = watcher.plot_values_time(\n",
    "    x[0],\n",
    "    x_axis=x[0].numpy(),\n",
    "    project_heads=False,#tuple(torch.where(W_H.abs()[0] > 5)[0].numpy()),\n",
    "    # reduce_dims={0: 0},\n",
    "    figsize=(10, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = watcher.plot_iss_time(\n",
    "    x[0],\n",
    "    x_axis=x[0].numpy(),\n",
    "    project_heads=False,#tuple(torch.where(W_H.abs()[0] > 5)[0].numpy()),\n",
    "    # reduce_dims={0: 0},\n",
    "    figsize=(10, 5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = watcher.plot_attention_matrices(\n",
    "    torch.arange(6),#.flip(0),#x[0],#\n",
    "    xlabels=list(map(str, np.arange(6))),\n",
    "    show_example=False,\n",
    "    # total=True,\n",
    "    cmap=\"seismic\",\n",
    "    share_cmap=False,\n",
    "    log_cmap=False,\n",
    "    causal_mask=False,\n",
    "    only_kernels=None,\n",
    "    # value_direction=0,\n",
    "    # all_but_first_value=False,\n",
    "    project_heads=False,#tuple(torch.where(W_H.abs()[0] > 5)[0].numpy()),\n",
    "    center_zero=True,\n",
    "    # index_selection=((-1, torch.arange(95, 100)), (-2, torch.arange(95, 100))),\n",
    "    cmap_example=\"tab10\",\n",
    "    figsize=(9, 10),\n",
    ")\n",
    "# fig.savefig(\n",
    "#     f\"cyclic_attention_{model_id}.pdf\",\n",
    "#     facecolor=(0, 0, 0, 0),\n",
    "#     bbox_inches=\"tight\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sainomore-JMySAefd-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
